---
title: "Swarm Simulation"
date: 2019-11-03
categories: [MARL, hackathon]
tags: [unity, c#, social-sciences]
---

#### Experimenting with emergent behaviour in 3D physics ecosystems.

This weekend I dedicated towards learning about swarm behaviour in animal groups. 
It all got started when I bumped into Sebastian Lague's [video]((https://www.youtube.com/watch?v=bqtqltqcQhw)) on boids.
<p align="center">
  <img src="https://camo.githubusercontent.com/cb8a79994f04c8c2ca31da7f49a91837e4ab2324/68747470733a2f2f692e696d6775722e636f6d2f513145343838752e706e67" width="77%" />
</p>
I was mesmerized and puzzled by behavioural patterns that can emerge in groups of primitive individuals. 
I thus went ahead and watched Iain Couzin's [talk](https://www.youtube.com/watch?v=lWHYFoFRY34) 
at [Cosyne conference](http://www.cosyne.org) on collective behavior in which he revealed some interesting observations 
from nature along with plausible reasons for swarming of various animal species. With an intention to dive deeper into 
the topic I have decided to enroll into [McGill Physics Hackathon](http://www.physics.mcgill.ca/hackathon) and encouraged my friends who are familiar with Unity Engine to join me.

From the start, I had the idea of applying Multi-Agent Reinforcement Learning [MARL]({% post_url 2019-09-23-marl-manifesto %}) 
techniques to model individual agents and Unity `ml-agents` library was a perfect interface for that. 
In `ml-agents` one creates an `Agent` that interacts with the environment and endows it with a `Brain` which is 
a parametrized policy that receives observation tuples collected by an actor and sends back an action to perform in the world. 
If you want to learn more about ml-agents, [this](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md) 
is the best way to start. 

Before jumping into coding, I have reviewed some literature on the topic. 
Some of the papers that I have skimmed through included [Reinforcement Learning Agents acquire Flocking and Symbiotic Behaviour in Simulated Ecosystems](https://www.mitpressjournals.org/doi/pdf/10.1162/isal_a_00148) 
from DeepMind, which suggested that we were on the right path and the swarming could emerge naturally as long 
as the incentives are aligned. In order to encourage our boids to swarm, we have introduced a `Predator` boid, 
whose goal was to hunt down the `Prey` boids. Following `ml-agents` interface, we set up each boid to have its own policy, keeping the representation between different species shared. 

#### State Space
In order to interact with environment, each boid has been given `Vision`, corresponding to the spherical region around it. 
Although this is far from the true eyesight of animals, due to hackathon's 24 hour format a sphere was good enough. 
[Here](https://www.earthlife.net/fish/images/anatomy/s-vision.gif) is how much fish can actually see. 
Boid agent's state space would thus consist of coordinates and velocities of the boids within the sight as well as 
types of those boids --- 0 for self, 1 for kin and 2 for other species type. 

#### Rewards
In order to determine rewards, we have constructed another sphere around the boid which we called 'predation' region. 
This region was instrumental to determining rewards of the agents. 
If the predation region of `Prey` and `Predator` boids would intersect, a reward of -1 and 1 would be given respectively. 
Since we have decided to avoid an explicit incentive for swarming, the reward in all other cases was left at 0. 
Below is an illustration of boids' vision and predation regions. 
<p align="center">
  <img src="/assets/boid_vision.png" width="77%" />
</p>

#### Action Space
The action space turned out to be the trickiest to get right. While not being very familiar with the way 
Unity manages physics, through trial and error we have arrived at something reasonably fluid:
```
    public float speed = 1;
    public override void AgentAction(float[] vectorAction, string textAction)
    {
        Vector3 controlSignal = Vector3.zero;
        controlSignal.x = vectorAction[0];
        controlSignal.z = vectorAction[1];
        controlSignal.y = vectorAction[2];
        rBody.AddForce(controlSignal * speed, ForceMode.Impulse);

        // Decrease the excessive spinning
        rBody.velocity = rBody.velocity * 0.9f;
        rBody.angularVelocity = rBody.angularVelocity * 0.9f;
    }
```
I am planning to revise this part after I get a bit more experience in Unity since the kind of movement produced 
by boids is still looking jagged.

#### Experiments
In the meantime, while I was setting up the 'brains' of the boids, my teammates were able to put an environment
together with some realistic physical properties and some cute fish models. It was the time to run the simulations!
In our first experiment, we have paired a `Prey` with a single `Predator` to test the validity of RL approach. 
The reward for the predator to be inversely proportional to the distance from the prey. 
That is, the closer the predator was to the prey, the bigger the reward it received and vice-versa. 
At first, the predator was dominating, but the prey was able to 'outrun' the predator as the training evolved. 
We concluded that this is due to prey's lower cross-sectional area and, therefore, less underwater drag.
After testing environment and fixing some unexpected bugs (like fish flying out of our virtual aquarium), we proceeded 
with our main simulation with 15 `Prey`s and 3 `Predator`s. After training for about an hour this what we saw:
<p align="center">
  <img src="/assets/boidz_chase.gif" width="77%" />
</p>
It got us by surprise that the prey has chosen to hide in the corner instead of schooling as we expected.
Apart from a possibility of a major bug in the environment or boid brains, our intuition tells us that it might 
actually be optimal in this particular environment as the prey learned to set up a living shield! 
Although we did not achieve out goal -- make the boids swarm -- in 24 hours, we are positive that after necessary fixes 
in the boid-environment interaction the collective behaviour is going to emerge.   

---

I treat hackathons as these rare opportunities to work on the projects you have always wanted to mess around with, but never really got time to.
On that note shout-outs to the organizers for putting together an awesome [event](http://www.physics.mcgill.ca/hackathon)!
You can check out other project submissions on [devpost](https://mcgill-physics-hackathon-2019.devpost.com/submissions).    

---

Apparently, we got nominated for the top 3 in the hackathon, 
but everyone felt sleep deprived so we left home before judging sessions...

